{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nupic.research.frameworks import vernon\n",
    "from copy import copy, deepcopy\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['ContinualLearningExperiment',\n",
       " 'MetaContinualLearningExperiment',\n",
       " 'RezeroedKWinnersGSCExperiment',\n",
       " 'SupervisedExperiment',\n",
       " 'VariedRezeroedKWinnersGSCExperiment',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'cl_experiment',\n",
       " 'common_experiments',\n",
       " 'components',\n",
       " 'experiment_utils',\n",
       " 'experiments',\n",
       " 'interfaces',\n",
       " 'meta_cl_experiment',\n",
       " 'mixins',\n",
       " 'network_utils',\n",
       " 'supervised_experiment']"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "dir(vernon)"
   ]
  },
  {
   "source": [
    "## Supervised Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 Acc: 0.00\n",
      "{'total_correct': 0, 'total_tested': 0, 'mean_loss': 0.0, 'mean_accuracy': 0.0, 'learning_rate': 0.01}\n",
      "Epoch: 2 Acc: 0.00\n",
      "{'total_correct': 0, 'total_tested': 0, 'mean_loss': 0.0, 'mean_accuracy': 0.0, 'learning_rate': 0.01}\n",
      "Epoch: 3 Acc: 0.12\n",
      "{'total_correct': 38, 'total_tested': 320, 'mean_loss': 44.676943969726565, 'mean_accuracy': 0.11875, 'learning_rate': 0.01}\n",
      "Epoch: 4 Acc: 0.20\n",
      "{'total_correct': 65, 'total_tested': 320, 'mean_loss': 10.410250091552735, 'mean_accuracy': 0.203125, 'learning_rate': 0.01}\n",
      "Epoch: 5 Acc: 0.12\n",
      "{'total_correct': 38, 'total_tested': 320, 'mean_loss': 4.339820861816406, 'mean_accuracy': 0.11875, 'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from nupic.research.frameworks.vernon import SupervisedExperiment\n",
    "\n",
    "supervised_test = dict(\n",
    "    # dataset -  using torchvision\n",
    "    dataset_class=datasets.CIFAR10,\n",
    "    dataset_args=dict(root=\"~/nta/datasets\", transform=transforms.ToTensor()),       # model - using torchvision\n",
    "    model_class=models.resnet18,\n",
    "    model_args=dict(num_classes=10, pretrained=False),\n",
    "    num_classes=10,\n",
    "    # experiment\n",
    "    distributed=False,\n",
    "    # hyperparameters\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    optimizer_args=dict(lr=1e-2),\n",
    "    optimizer_class=torch.optim.Adam,\n",
    "    # for debugging\n",
    "    batches_in_epoch=10,\n",
    "    batches_in_epoch_val=10,\n",
    ")\n",
    "\n",
    "def run(experiment_class, config):\n",
    "    exp = experiment_class()\n",
    "    exp.setup_experiment(config)\n",
    "\n",
    "    epoch = 0\n",
    "    while not exp.should_stop():\n",
    "        epoch += 1\n",
    "        results = exp.run_epoch()\n",
    "        print(f\"Epoch: {epoch} Acc: {results['mean_accuracy']:.2f}\")\n",
    "        print(results)\n",
    "\n",
    "run(SupervisedExperiment, config=supervised_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 Acc: 0.00\n",
      "Epoch: 2 Acc: 0.00\n",
      "Epoch: 3 Acc: 0.12\n",
      "Epoch: 4 Acc: 0.20\n",
      "Epoch: 5 Acc: 0.12\n"
     ]
    }
   ],
   "source": [
    "from nupic.research.frameworks.vernon import mixins\n",
    "\n",
    "class CutMixSupervisedExperiment(mixins.CutMix,\n",
    "                                 SupervisedExperiment):\n",
    "    pass\n",
    "\n",
    "supervised_test_v2 = deepcopy(supervised_test)\n",
    "supervised_test_v2.update(\n",
    "    experiment_class=CutMixSupervisedExperiment,\n",
    "    mixup_beta=1.0,\n",
    "    cutmix_prob=0.8,\n",
    ")\n",
    "\n",
    "run(SupervisedExperiment, config=supervised_test_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['CompositeLoss',\n",
       " 'ConstrainParameters',\n",
       " 'CutMix',\n",
       " 'CutMixKnowledgeDistillation',\n",
       " 'DelayLoadCheckpoint',\n",
       " 'ElasticWeightConsolidation',\n",
       " 'ExportModel',\n",
       " 'ExtraValidationsPerEpoch',\n",
       " 'KnowledgeDistillation',\n",
       " 'KnowledgeDistillationCL',\n",
       " 'LRRangeTest',\n",
       " 'LegacyImagenetConfig',\n",
       " 'LoadPreprocessedData',\n",
       " 'LogBackpropStructure',\n",
       " 'LogCovariance',\n",
       " 'LogEveryLearningRate',\n",
       " 'LogEveryLoss',\n",
       " 'MaxupPerSample',\n",
       " 'MaxupStandard',\n",
       " 'MultiCycleLR',\n",
       " 'OnlineMetaLearning',\n",
       " 'Profile',\n",
       " 'ProfileAutograd',\n",
       " 'PruneLowMagnitude',\n",
       " 'PruneLowSNR',\n",
       " 'QuantizationAware',\n",
       " 'ReduceLRAfterTask',\n",
       " 'RegularizeLoss',\n",
       " 'RezeroWeights',\n",
       " 'SaveFinalCheckpoint',\n",
       " 'StepBasedLogging',\n",
       " 'UpdateBoostStrength',\n",
       " 'VaryBatchSize',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'composite_loss',\n",
       " 'constrain_parameters',\n",
       " 'create_lr_test_experiment',\n",
       " 'cutmix',\n",
       " 'delay_load_checkpoint',\n",
       " 'ewc',\n",
       " 'export_model',\n",
       " 'extra_validations_per_epoch',\n",
       " 'knowledge_distillation',\n",
       " 'legacy_imagenet_config',\n",
       " 'load_preprocessed_data',\n",
       " 'log_backprop_structure',\n",
       " 'log_covariance',\n",
       " 'log_every_learning_rate',\n",
       " 'log_every_loss',\n",
       " 'lr_range_test',\n",
       " 'maxup',\n",
       " 'multi_cycle_lr',\n",
       " 'oml',\n",
       " 'profile',\n",
       " 'profile_autograd',\n",
       " 'prune_low_magnitude',\n",
       " 'prune_low_snr',\n",
       " 'quantization_aware',\n",
       " 'reduce_lr_after_task',\n",
       " 'regularize_loss',\n",
       " 'rezero_weights',\n",
       " 'save_final_checkpoint',\n",
       " 'step_based_logging',\n",
       " 'update_boost_strength',\n",
       " 'vary_batch_size']"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "dir(mixins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch: 1 Acc: 0.00\n",
      "Epoch: 2 Acc: 0.00\n",
      "Epoch: 3 Acc: 0.12\n",
      "Epoch: 4 Acc: 0.25\n",
      "Epoch: 5 Acc: 0.25\n"
     ]
    }
   ],
   "source": [
    "# models and datasets available - not part of Vernon\n",
    "from nupic.research.frameworks.pytorch import models as local_models\n",
    "from nupic.research.frameworks.pytorch import datasets as local_datasets\n",
    "\n",
    "supervised_test_v3 = deepcopy(supervised_test)\n",
    "supervised_test_v3.update(\n",
    "    # dataset - alternative using torchvision factory\n",
    "    # includes base transforms as transforming to tensor and normalization\n",
    "    dataset_class=local_datasets.torchvisiondataset,\n",
    "    dataset_args=dict(root=\"~/nta/datasets\", dataset_name=\"CIFAR10\"),\n",
    "    # can use local models available\n",
    "    model_class=local_models.resnet9,\n",
    "    model_args=dict(num_classes=10),\n",
    ")\n",
    "\n",
    "run(SupervisedExperiment, config=supervised_test_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'download_gsc_data',\n",
       " 'gsc_factory',\n",
       " 'imagenet',\n",
       " 'imagenet_factory',\n",
       " 'omniglot',\n",
       " 'preprocessed_gsc',\n",
       " 'torchvision_factory',\n",
       " 'torchvisiondataset']"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "dir(local_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['DenseNetCIFAR',\n",
       " 'LeSparseNet',\n",
       " 'MetaContinualLearningMLP',\n",
       " 'MobileNetV1',\n",
       " 'NoSoDenseNetCIFAR',\n",
       " 'OMLNetwork',\n",
       " 'OmniglotCNN',\n",
       " 'ResNet',\n",
       " 'StandardMLP',\n",
       " 'VGGSparseNet',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'common_models',\n",
       " 'le_sparse_net',\n",
       " 'mobile_net_v1_sparse_depth',\n",
       " 'mobile_net_v1_sparse_point',\n",
       " 'mobilenetv1',\n",
       " 'not_so_densenet',\n",
       " 'pretrained_models',\n",
       " 'resnet50_swsl',\n",
       " 'resnet9',\n",
       " 'resnet_models',\n",
       " 'resnets',\n",
       " 'resnext101_32x16d_wsl',\n",
       " 'resnext101_32x48d_wsl',\n",
       " 'resnext50_32x4d_swsl',\n",
       " 'se_resnet50',\n",
       " 'se_resnext50_32x4d',\n",
       " 'separable_convolution2d',\n",
       " 'vgg19_dense_net',\n",
       " 'vgg19_sparse_net',\n",
       " 'vgg_sparse_net',\n",
       " 'xception']"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "dir(local_models)"
   ]
  },
  {
   "source": [
    "## Continual Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-01-21 10:36:51,469\tINFO resource_spec.py:212 -- Starting Ray with 8.69 GiB memory available for workers and up to 4.36 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "{'checkpoint_at_end': False,\n",
      " 'checkpoint_freq': 0,\n",
      " 'checkpoint_score_attr': None,\n",
      " 'config': {'batch_size': 32,\n",
      "            'batches_in_epoch': 30,\n",
      "            'batches_in_epoch_val': 30,\n",
      "            'dataset_args': {'dataset_name': 'MNIST', 'root': '~/nta/datasets'},\n",
      "            'dataset_class': <function torchvisiondataset at 0x7fe4a278d158>,\n",
      "            'dist_port': 56312,\n",
      "            'distributed': False,\n",
      "            'epochs': 5,\n",
      "            'epochs_to_validate': [],\n",
      "            'evaluation_metrics': ['eval_current_task',\n",
      "                                   'eval_all_visited_tasks'],\n",
      "            'experiment_class': <class '__main__.ReduceLRContinualLearningExperiment'>,\n",
      "            'log_level': 'INFO',\n",
      "            'model_args': {'hidden_sizes': (50, 50, 50),\n",
      "                           'input_size': (28, 28),\n",
      "                           'num_classes': 10},\n",
      "            'model_class': <class 'nupic.research.frameworks.pytorch.models.common_models.StandardMLP'>,\n",
      "            'num_classes': 10,\n",
      "            'num_gpus': 0,\n",
      "            'num_tasks': 5,\n",
      "            'optimizer_args': {'lr': 0.01, 'momentum': 0.9, 'nesterov': False},\n",
      "            'optimizer_class': <class 'torch.optim.sgd.SGD'>,\n",
      "            'reuse_actors': False,\n",
      "            'workers': 4},\n",
      " 'export_formats': None,\n",
      " 'global_checkpoint_period': 10,\n",
      " 'keep_checkpoints_num': None,\n",
      " 'local_dir': None,\n",
      " 'loggers': None,\n",
      " 'max_failures': 0,\n",
      " 'name': None,\n",
      " 'num_samples': 1,\n",
      " 'progress_reporter': None,\n",
      " 'queue_trials': False,\n",
      " 'raise_on_failed_trial': True,\n",
      " 'ray_auto_init': True,\n",
      " 'resources_per_trial': None,\n",
      " 'restore': None,\n",
      " 'resume': False,\n",
      " 'return_trials': False,\n",
      " 'reuse_actors': False,\n",
      " 'run_or_experiment': <class 'nupic.research.frameworks.vernon.trainables.RemoteProcessTrainable'>,\n",
      " 'scheduler': None,\n",
      " 'search_alg': None,\n",
      " 'server_port': 4321,\n",
      " 'stop': {},\n",
      " 'sync_on_checkpoint': True,\n",
      " 'sync_to_cloud': None,\n",
      " 'sync_to_driver': None,\n",
      " 'trial_executor': None,\n",
      " 'trial_name_creator': None,\n",
      " 'upload_dir': None,\n",
      " 'verbose': 2,\n",
      " 'with_server': False}\n",
      "2021-01-21 10:36:51,797\tINFO services.py:1123 -- View the Ray dashboard at \u001b[1m\u001b[32m192.168.0.10:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 20.7/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/16 CPUs, 0/0 GPUs, 0.0/8.69 GiB heap, 0.0/2.98 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                  </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2m\u001b[36m(pid=85446)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Training...\n",
      "\u001b[2m\u001b[36m(pid=85443)\u001b[0m 2021-01-21 10:37:12,352\tINFO trainable.py:180 -- _setup took 14.472 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=85443)\u001b[0m INFO:RemoteProcessTrainable:Pre-Experiment Result: None\n",
      "Result for RemoteProcessTrainable_00000:\n",
      "  date: 2021-01-21_10-37-13\n",
      "  done: false\n",
      "  eval_all_visited_tasks__mean_accuracy: 0.9989583333333333\n",
      "  eval_all_visited_tasks__mean_loss: 0.0030511913200219473\n",
      "  eval_all_visited_tasks__total_correct: 959\n",
      "  eval_all_visited_tasks__total_tested: 960\n",
      "  eval_current_task__mean_accuracy: 1.0\n",
      "  eval_current_task__mean_loss: 0.002297080059846242\n",
      "  eval_current_task__total_correct: 960\n",
      "  eval_current_task__total_tested: 960\n",
      "  experiment_id: a5469d764fc74d1e8ed983172aecacac\n",
      "  experiment_tag: '0'\n",
      "  hostname: Lucas-MacBook-Pro.local\n",
      "  iterations_since_restore: 1\n",
      "  learning_rate: 0.01\n",
      "  node_ip: 192.168.0.10\n",
      "  pid: 85443\n",
      "  time_since_restore: 0.8513369560241699\n",
      "  time_this_iter_s: 0.8513369560241699\n",
      "  time_total_s: 0.8513369560241699\n",
      "  timestamp: 1611254233\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '00000'\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=85446)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Setting learning rate to 0.00\n",
      "\u001b[2m\u001b[36m(pid=85443)\u001b[0m INFO:RemoteProcessTrainable:End Iteration Result: {'learning_rate': 0.01}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 20.8/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/16 CPUs, 0/0 GPUs, 0.0/8.69 GiB heap, 0.0/2.98 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_00000</td><td>RUNNING </td><td>192.168.0.10:85443</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.851337</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2m\u001b[36m(pid=85446)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Training...\n",
      "\u001b[2m\u001b[36m(pid=85446)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Setting learning rate to 0.00\n",
      "\u001b[2m\u001b[36m(pid=85446)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Training...\n",
      "\u001b[2m\u001b[36m(pid=85443)\u001b[0m INFO:RemoteProcessTrainable:End Iteration Result: {'learning_rate': 0.001}\n",
      "\u001b[2m\u001b[36m(pid=85446)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Setting learning rate to 0.00\n",
      "\u001b[2m\u001b[36m(pid=85446)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Training...\n",
      "\u001b[2m\u001b[36m(pid=85443)\u001b[0m INFO:RemoteProcessTrainable:End Iteration Result: {'learning_rate': 0.001}\n",
      "\u001b[2m\u001b[36m(pid=85446)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Setting learning rate to 0.00\n",
      "\u001b[2m\u001b[36m(pid=85446)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Training...\n",
      "\u001b[2m\u001b[36m(pid=85443)\u001b[0m INFO:RemoteProcessTrainable:End Iteration Result: {'learning_rate': 0.001}\n",
      "Result for RemoteProcessTrainable_00000:\n",
      "  date: 2021-01-21_10-37-16\n",
      "  done: true\n",
      "  eval_all_visited_tasks__mean_accuracy: 0.17604166666666668\n",
      "  eval_all_visited_tasks__mean_loss: 8.390658569335937\n",
      "  eval_all_visited_tasks__total_correct: 169\n",
      "  eval_all_visited_tasks__total_tested: 960\n",
      "  eval_current_task__mean_accuracy: 0.9614583333333333\n",
      "  eval_current_task__mean_loss: 0.1368893305460612\n",
      "  eval_current_task__total_correct: 923\n",
      "  eval_current_task__total_tested: 960\n",
      "  experiment_id: a5469d764fc74d1e8ed983172aecacac\n",
      "  experiment_tag: '0'\n",
      "  hostname: Lucas-MacBook-Pro.local\n",
      "  iterations_since_restore: 5\n",
      "  learning_rate: 0.001\n",
      "  node_ip: 192.168.0.10\n",
      "  pid: 85443\n",
      "  time_since_restore: 3.7205045223236084\n",
      "  time_this_iter_s: 0.7071397304534912\n",
      "  time_total_s: 3.7205045223236084\n",
      "  timestamp: 1611254236\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 20.8/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/8.69 GiB heap, 0.0/2.98 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                  </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          3.7205</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "**** Trial ended\n"
     ]
    }
   ],
   "source": [
    "from nupic.research.frameworks.vernon import ContinualLearningExperiment\n",
    "from nupic.research.frameworks.pytorch.models import StandardMLP\n",
    "\n",
    "class ReduceLRContinualLearningExperiment(mixins.ReduceLRAfterTask,\n",
    "                                          ContinualLearningExperiment):\n",
    "    pass\n",
    "\n",
    "cl_mnist = dict(\n",
    "    # specific to continual learning\n",
    "    distributed=False,\n",
    "    experiment_class=ReduceLRContinualLearningExperiment,\n",
    "    num_classes=10,\n",
    "    num_tasks=5,\n",
    "    evaluation_metrics=[\n",
    "        \"eval_current_task\",\n",
    "        \"eval_all_visited_tasks\",\n",
    "    ],\n",
    "    # dataset\n",
    "    dataset_class=torchvisiondataset,\n",
    "    dataset_args=dict(root=\"~/nta/datasets\", dataset_name=\"MNIST\"),    \n",
    "    # regular experiments\n",
    "    model_class=StandardMLP,\n",
    "    model_args=dict(\n",
    "        input_size=(28, 28), num_classes=10, hidden_sizes=(50, 50, 50)\n",
    "    ),\n",
    "    # hyperparameters\n",
    "    epochs_to_validate=[],\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    optimizer_class=torch.optim.SGD,\n",
    "    optimizer_args=dict(lr=0.01, momentum=0.9, nesterov=False),\n",
    "    # for debugging\n",
    "    batches_in_epoch=30,\n",
    "    batches_in_epoch_val=30\n",
    ")\n",
    "\n",
    "from nupic.research.frameworks.vernon.run_with_raytune import run_single_instance\n",
    "run_single_instance(cl_mnist)\n"
   ]
  },
  {
   "source": [
    "## Meta-Continual Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-01-21 10:41:45,617\tINFO resource_spec.py:212 -- Starting Ray with 9.47 GiB memory available for workers and up to 4.74 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "{'checkpoint_at_end': False,\n",
      " 'checkpoint_freq': 0,\n",
      " 'checkpoint_score_attr': None,\n",
      " 'config': {'batch_size': 5,\n",
      "            'dataset_args': {'root': '~/nta/datasets'},\n",
      "            'dataset_class': <function omniglot at 0x7fe4a278d1e0>,\n",
      "            'dist_port': 56649,\n",
      "            'distributed': False,\n",
      "            'epochs': 2,\n",
      "            'experiment_class': <class 'nupic.research.frameworks.vernon.experiments.meta_cl_experiment.MetaContinualLearningExperiment'>,\n",
      "            'fast_params': ['adaptation.*'],\n",
      "            'log_level': 'INFO',\n",
      "            'model_args': {'num_classes': 50},\n",
      "            'model_class': <class 'nupic.research.frameworks.pytorch.models.common_models.OMLNetwork'>,\n",
      "            'num_batches_train': 1,\n",
      "            'num_classes': 50,\n",
      "            'num_gpus': 0,\n",
      "            'num_tasks_per_epoch': 10,\n",
      "            'optimizer_args': {'lr': 0.0001},\n",
      "            'optimizer_class': <class 'torch.optim.adam.Adam'>,\n",
      "            'reuse_actors': False,\n",
      "            'test_train_params': ['adaptation.*'],\n",
      "            'workers': 4},\n",
      " 'export_formats': None,\n",
      " 'global_checkpoint_period': 10,\n",
      " 'keep_checkpoints_num': None,\n",
      " 'local_dir': None,\n",
      " 'loggers': None,\n",
      " 'max_failures': 0,\n",
      " 'name': None,\n",
      " 'num_samples': 1,\n",
      " 'progress_reporter': None,\n",
      " 'queue_trials': False,\n",
      " 'raise_on_failed_trial': True,\n",
      " 'ray_auto_init': True,\n",
      " 'resources_per_trial': None,\n",
      " 'restore': None,\n",
      " 'resume': False,\n",
      " 'return_trials': False,\n",
      " 'reuse_actors': False,\n",
      " 'run_or_experiment': <class 'nupic.research.frameworks.vernon.trainables.RemoteProcessTrainable'>,\n",
      " 'scheduler': None,\n",
      " 'search_alg': None,\n",
      " 'server_port': 4321,\n",
      " 'stop': {},\n",
      " 'sync_on_checkpoint': True,\n",
      " 'sync_to_cloud': None,\n",
      " 'sync_to_driver': None,\n",
      " 'trial_executor': None,\n",
      " 'trial_name_creator': None,\n",
      " 'upload_dir': None,\n",
      " 'verbose': 2,\n",
      " 'with_server': False}\n",
      "2021-01-21 10:41:45,929\tINFO services.py:1123 -- View the Ray dashboard at \u001b[1m\u001b[32m192.168.0.10:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 19.9/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/16 CPUs, 0/0 GPUs, 0.0/9.47 GiB heap, 0.0/3.27 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                  </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2m\u001b[36m(pid=85800)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=85794)\u001b[0m 2021-01-21 10:42:31,484\tINFO trainable.py:180 -- _setup took 40.720 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=85794)\u001b[0m INFO:RemoteProcessTrainable:Pre-Experiment Result: None\n",
      "\u001b[2m\u001b[36m(pid=85800)\u001b[0m INFO:MetaContinualLearningExperiment:Setup: fast_param_names=['adaptation.0.weight', 'adaptation.0.bias']\n",
      "Result for RemoteProcessTrainable_00000:\n",
      "  date: 2021-01-21_10-43-11\n",
      "  done: false\n",
      "  experiment_id: 9e023309f1dc4c0eaa5a73df4c92f6af\n",
      "  experiment_tag: '0'\n",
      "  hostname: Lucas-MacBook-Pro.local\n",
      "  iterations_since_restore: 1\n",
      "  learning_rate: 0.0001\n",
      "  mean_accuracy: 0.10144927536231885\n",
      "  mean_loss: 70.39163970947266\n",
      "  neg_mean_loss: -70.39163970947266\n",
      "  node_ip: 192.168.0.10\n",
      "  pid: 85794\n",
      "  time_since_restore: 39.8740611076355\n",
      "  time_this_iter_s: 39.8740611076355\n",
      "  time_total_s: 39.8740611076355\n",
      "  timestamp: 1611254591\n",
      "  timesteps_since_restore: 0\n",
      "  total_correct: 7\n",
      "  total_tested: 69\n",
      "  training_iteration: 1\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 20.9/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/16 CPUs, 0/0 GPUs, 0.0/9.47 GiB heap, 0.0/3.27 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_00000</td><td>RUNNING </td><td>192.168.0.10:85794</td><td style=\"text-align: right;\">0.101449</td><td style=\"text-align: right;\">70.3916</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         39.8741</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2m\u001b[36m(pid=85794)\u001b[0m INFO:RemoteProcessTrainable:End Iteration Result: {'total_correct': 7, 'total_tested': 69, 'learning_rate': 0.0001, 'validation_loss': 70.39163970947266, 'validation_accuracy': 0.10144927536231885}\n",
      "Result for RemoteProcessTrainable_00000:\n",
      "  date: 2021-01-21_10-43-20\n",
      "  done: true\n",
      "  experiment_id: 9e023309f1dc4c0eaa5a73df4c92f6af\n",
      "  experiment_tag: '0'\n",
      "  hostname: Lucas-MacBook-Pro.local\n",
      "  iterations_since_restore: 2\n",
      "  learning_rate: 0.0001\n",
      "  mean_accuracy: 0.08695652173913043\n",
      "  mean_loss: 28.518835067749023\n",
      "  neg_mean_loss: -28.518835067749023\n",
      "  node_ip: 192.168.0.10\n",
      "  pid: 85794\n",
      "  time_since_restore: 49.49658703804016\n",
      "  time_this_iter_s: 9.622525930404663\n",
      "  time_total_s: 49.49658703804016\n",
      "  timestamp: 1611254600\n",
      "  timesteps_since_restore: 0\n",
      "  total_correct: 6\n",
      "  total_tested: 69\n",
      "  training_iteration: 2\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 20.9/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/9.47 GiB heap, 0.0/3.27 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                  </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">      acc</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0869565</td><td style=\"text-align: right;\">28.5188</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         49.4966</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2m\u001b[36m(pid=85794)\u001b[0m INFO:RemoteProcessTrainable:End Iteration Result: {'total_correct': 6, 'total_tested': 69, 'learning_rate': 0.0001, 'validation_loss': 28.518835067749023, 'validation_accuracy': 0.08695652173913043}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 20.9/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/9.47 GiB heap, 0.0/3.27 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                  </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">      acc</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0869565</td><td style=\"text-align: right;\">28.5188</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         49.4966</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "**** Trial ended\n"
     ]
    }
   ],
   "source": [
    "# ray running in local, if bug, needs to manually shutdown # FIXME\n",
    "import ray\n",
    "ray.shutdown()\n",
    "\n",
    "# networks and datasets under pytorch\n",
    "from nupic.research.frameworks.vernon import MetaContinualLearningExperiment\n",
    "from nupic.research.frameworks.pytorch.datasets import omniglot\n",
    "from nupic.research.frameworks.pytorch.models import OMLNetwork\n",
    "\n",
    "meta_cl_omniglot = dict(\n",
    "    # experiment\n",
    "    experiment_class=MetaContinualLearningExperiment,\n",
    "    distributed=False,\n",
    "    # dataset\n",
    "    dataset_class=omniglot,\n",
    "    dataset_args=dict(root=\"~/nta/datasets\"),\n",
    "    # model\n",
    "    model_class=OMLNetwork,\n",
    "    model_args=dict(num_classes=50),\n",
    "    fast_params=[\"adaptation.*\"],\n",
    "    test_train_params=[\"adaptation.*\"],\n",
    "    # hyperparameters\n",
    "    batch_size=5,\n",
    "    num_batches_train=1,\n",
    "    epochs=2,\n",
    "    num_tasks_per_epoch=10,\n",
    "    num_classes=50,\n",
    "    optimizer_args=dict(lr=1e-4),\n",
    "    optimizer_class=torch.optim.Adam,\n",
    ")\n",
    "\n",
    "# Use one of the existing run functions\n",
    "run_single_instance(meta_cl_omniglot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}