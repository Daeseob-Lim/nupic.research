# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------
[DEFAULT]
; Uncomment to save results to S3
;upload_dir = "s3://bucketname/ray/whydense/gsc"
;sync_function = "aws s3 sync `dirname {local_dir}` {remote_dir}/`basename $(dirname {local_dir})`"

path = ~/nta/results/gsc
data_dir = ~/nta/data/gsc_preprocessed
verbose = 2
num_classes = 12
checkpoint_freq = 1
checkpoint_at_end = False
gpu_percentage = 1.0

; Uncomment to average over multiple seeds
;repetitions = 1
;seed = 42
repetitions = 8
seed = tune.sample_from(lambda spec: np.random.randint(1, 10000))

iterations = 30             # Number of training epochs
batch_size = 16
batches_in_epoch = 5121
test_batch_size = 1000
num_workers=4

learning_rate = 0.01
weight_decay = 0.01
learning_rate_factor = 0.9
use_batch_norm = True
momentum = 0.0
boost_strength = 1.5
boost_strength_factor = 0.9
linear_n = 2000
linear_percent_on = 1.0
weight_sparsity = (0.50, )
cnn_weight_sparsity = (1.0, )
k_inference_factor = 1.0

log_interval = 400         # how many minibatches to wait before logging
test_noise_every_epoch = False # If False, will only test noise at end

background_noise_dir = _background_noise_

optimizer = SGD

; Learning Rate Scheduler. See "torch.optim.lr_scheduler" for valid class names
lr_scheduler = StepLR

model_type = le_sparse
activation_fct_before_max_pool = True
dropout = 0.0

input_shape = (1, 32, 32)

;[quick]
;linear_n = (100,)
;linear_percent_on = (0.1,)
;cnn_out_channels = (8, 8)
;cnn_percent_on = (0.095, 0.125)
;cnn_weight_sparsity = (1.0, 1.0)
;k_inference_factor = 1.5
;min_epoch_for_checkpoint = 3
;iterations = 2
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.9
;learning_rate = 0.01
;momentum = 0.0
;weight_sparsity = (0.4,)
;dropout = 0.0
;log_interval = 400
;batches_in_epoch = 3
;batch_size = 16
;model_type = le_sparse

# This one has 1000 linear units and 10% weight density.
# Reducing k inf factor to 1.0. This gets around 97% accuracy and
# has 12,471 robustness.
# Params: 194,388
[sparseCNN2Base]
cnn_out_channels = (64, 64)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.2)
linear_n = (1000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.1,)
k_inference_factor = 1.0
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True
gpu_percentage = 0.25

# Accuracy:
# Params: 193,372
[L1]
cnn_out_channels = (64, 96)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.12)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.043,)
k_inference_factor = 1.0
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True

# Accuracy:
# Params: 179,820
[L1_S1]
cnn_out_channels = (64, 96)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.1)
linear_n = (1500,)
linear_percent_on = (0.1,)
weight_sparsity = (0.04,)
k_inference_factor = 1.0
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True

# Accuracy:
# Params: 189,392
[L2]
cnn_out_channels = (64, 96)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.12)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.03,)
k_inference_factor = 1.0
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True

# Accuracy:
# Params:
[L2_S1]
cnn_out_channels = (64, 96)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.08)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.03,)
k_inference_factor = 1.0
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True

# Accuracy:
# Params:
[L2_S2]
cnn_out_channels = (64, 96)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (0.5, 0.08)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.025,)
k_inference_factor = 1.0
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True

# Accuracy:
# Params:
[L2_S3]
cnn_out_channels = (64, 96)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.08)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.025,)
k_inference_factor = 1.0
boost_strength = 1.5
boost_strength_factor = 0.9
learning_rate_factor = 0.9
learning_rate = 0.01
momentum = 0.0
dropout = 0.0
log_interval = 400
batches_in_epoch = 5121
batch_size = 16
model_type = le_sparse
activation_fct_before_max_pool = True

# Accuracy:
# Params:
[L3]
cnn_out_channels = (64, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.08)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.023,)

# Accuracy:
# Params:
[L3_S1]
cnn_out_channels = (64, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.08)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.021,)

# Accuracy:
# Params:
[L3_S2]
cnn_out_channels = (64, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.07)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.0215,)

# Accuracy:
# Params:
[L3_S3]
cnn_out_channels = (64, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.06)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.0215,)

# Accuracy:
# Params:
[L3_S4]
cnn_out_channels = (64, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.055)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.0215,)

# Accuracy:
# Params:
[L3_S5]
cnn_out_channels = (64, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.05)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.0215,)

# Accuracy:
# Params:
[L3_S6]
cnn_out_channels = (64, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.045)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.0215,)

# Accuracy:
# Params:
[L3_S7]
cnn_out_channels = (64, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.04)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.0215,)

# Accuracy:
# Params:
[L3_S8]
cnn_out_channels = (64, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.035)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.0215,)




# Accuracy:
# Params:
[L4]
cnn_out_channels = (96, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.055)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.02,)

# Accuracy:
# Params:
[L4_S1]
cnn_out_channels = (96, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.055)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.018,)

# Accuracy:
# Params:
[L4_S2]
cnn_out_channels = (96, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.05)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.02,)

# Accuracy:
# Params:
[L4_S3]
cnn_out_channels = (96, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.045)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.02,)

# Accuracy:
# Params:
[L4_S4]
cnn_out_channels = (96, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.04)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.02,)

# Accuracy:
# Params:
[L4_S5]
cnn_out_channels = (96, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.035)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.02,)

# Accuracy:
# Params:
[L4_S6]
cnn_out_channels = (96, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.03)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.02,)

# Accuracy:
# Params:
[L4_S7]
cnn_out_channels = (96, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.025)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.02,)

# Accuracy:
# Params:
[L4_S8]
cnn_out_channels = (96, 128)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.0225)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.02,)



# Accuracy:
# Params:
[L5]
cnn_out_channels = (96, 160)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.05)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.016,)

# Accuracy:
# Params:
[L5_1]
cnn_out_channels = (96, 160)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.04)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.016,)

# Accuracy:
# Params:
[L5_2]
cnn_out_channels = (96, 160)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.05)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.014,)

# Accuracy:
# Params:
[L5_3]
cnn_out_channels = (96, 160)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.04)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.014,)

# Accuracy:
# Params:
[L5_4]
cnn_out_channels = (96, 160)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.035)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.014,)

[L6]
cnn_out_channels = (96, 160)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.03)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.014,)

[L6_1]
cnn_out_channels = (96, 160)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.025)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.014,)

[L6_2]
cnn_out_channels = (96, 160)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.02)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.014,)

[L6_3]
cnn_out_channels = (96, 160)
cnn_percent_on = (0.095, 0.125)
cnn_weight_sparsity = (1.0, 0.02)
linear_n = (2000,)
linear_percent_on = (0.1,)
weight_sparsity = (0.014,)
