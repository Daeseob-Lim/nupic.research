{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit"
  },
  "interpreter": {
   "hash": "a9f20cd5db126871b659419f7d0ee0d6bff40ece63ace9fb476b1b26d1698350"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nupic.research.frameworks import vernon\n",
    "from copy import copy, deepcopy\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(vernon)"
   ]
  },
  {
   "source": [
    "## Supervised Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['RezeroedKWinnersGSCExperiment',\n",
       " 'SupervisedExperiment',\n",
       " 'VariedRezeroedKWinnersGSCExperiment',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'common_experiments',\n",
       " 'components',\n",
       " 'experiment_utils',\n",
       " 'experiments',\n",
       " 'interfaces',\n",
       " 'mixins',\n",
       " 'network_utils',\n",
       " 'supervised_experiment']"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "import nupic.research\n",
    "dir(nupic.research.frameworks.vernon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nupic.research.frameworks.vernon import SupervisedExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "Epoch: 1 Acc: 0.00\n",
      "{'total_correct': 0, 'total_tested': 0, 'mean_loss': 0.0, 'mean_accuracy': 0.0, 'learning_rate': 0.01}\n",
      "Epoch: 2 Acc: 0.00\n",
      "{'total_correct': 0, 'total_tested': 0, 'mean_loss': 0.0, 'mean_accuracy': 0.0, 'learning_rate': 0.01}\n",
      "Epoch: 3 Acc: 0.10\n",
      "{'total_correct': 33, 'total_tested': 320, 'mean_loss': 18.545791625976562, 'mean_accuracy': 0.103125, 'learning_rate': 0.01}\n",
      "Epoch: 4 Acc: 0.09\n",
      "{'total_correct': 28, 'total_tested': 320, 'mean_loss': 17.907684326171875, 'mean_accuracy': 0.0875, 'learning_rate': 0.01}\n",
      "Epoch: 5 Acc: 0.21\n",
      "{'total_correct': 66, 'total_tested': 320, 'mean_loss': 12.975172424316407, 'mean_accuracy': 0.20625, 'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from nupic.research.frameworks.vernon import SupervisedExperiment\n",
    "\n",
    "supervised_test = dict(\n",
    "    # dataset -  using torchvision\n",
    "    dataset_class=datasets.CIFAR10,\n",
    "    dataset_args=dict(root=\"~/nta/datasets\", transform=transforms.ToTensor()),       # model - using torchvision\n",
    "    model_class=models.resnet18,\n",
    "    model_args=dict(num_classes=10, pretrained=False),\n",
    "    num_classes=10,\n",
    "    # experiment\n",
    "    distributed=False,\n",
    "    # hyperparameters\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    optimizer_args=dict(lr=1e-2),\n",
    "    optimizer_class=torch.optim.Adam,\n",
    "    # for debugging\n",
    "    batches_in_epoch=10,\n",
    "    batches_in_epoch_val=10,\n",
    ")\n",
    "\n",
    "def run(experiment_class, config):\n",
    "    exp = experiment_class()\n",
    "    exp.setup_experiment(config)\n",
    "\n",
    "    epoch = 0\n",
    "    while not exp.should_stop():\n",
    "        epoch += 1\n",
    "        results = exp.run_epoch()\n",
    "        print(f\"Epoch: {epoch} Acc: {results['mean_accuracy']:.2f}\")\n",
    "        print(results)\n",
    "\n",
    "run(SupervisedExperiment, config=supervised_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "Epoch: 1 Acc: 0.00\n",
      "{'total_correct': 0, 'total_tested': 0, 'mean_loss': 0.0, 'mean_accuracy': 0.0, 'learning_rate': 0.01}\n",
      "Epoch: 2 Acc: 0.00\n",
      "{'total_correct': 0, 'total_tested': 0, 'mean_loss': 0.0, 'mean_accuracy': 0.0, 'learning_rate': 0.01}\n",
      "Epoch: 3 Acc: 0.10\n",
      "{'total_correct': 33, 'total_tested': 320, 'mean_loss': 18.545791625976562, 'mean_accuracy': 0.103125, 'learning_rate': 0.01}\n",
      "Epoch: 4 Acc: 0.09\n",
      "{'total_correct': 28, 'total_tested': 320, 'mean_loss': 17.907684326171875, 'mean_accuracy': 0.0875, 'learning_rate': 0.01}\n",
      "Epoch: 5 Acc: 0.21\n",
      "{'total_correct': 66, 'total_tested': 320, 'mean_loss': 12.975172424316407, 'mean_accuracy': 0.20625, 'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "from nupic.research.frameworks.vernon import mixins\n",
    "\n",
    "class CutMixSupervisedExperiment(mixins.CutMix,\n",
    "                                 SupervisedExperiment):\n",
    "    pass\n",
    "\n",
    "supervised_test_v2 = deepcopy(supervised_test)\n",
    "supervised_test_v2.update(\n",
    "    experiment_class=CutMixSupervisedExperiment,\n",
    "    mixup_beta=1.0,\n",
    "    cutmix_prob=0.8,\n",
    ")\n",
    "\n",
    "run(SupervisedExperiment, config=supervised_test_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['CompositeLoss',\n",
       " 'ConfigureOptimizerParamGroups',\n",
       " 'ConstrainParameters',\n",
       " 'CutMix',\n",
       " 'CutMixKnowledgeDistillation',\n",
       " 'DelayLoadCheckpoint',\n",
       " 'ExportModel',\n",
       " 'ExtraValidationsPerEpoch',\n",
       " 'GradientMetrics',\n",
       " 'KnowledgeDistillation',\n",
       " 'KnowledgeDistillationCL',\n",
       " 'LRRangeTest',\n",
       " 'LegacyImagenetConfig',\n",
       " 'LoadPreprocessedData',\n",
       " 'LogBackpropStructure',\n",
       " 'LogCovariance',\n",
       " 'LogEveryLearningRate',\n",
       " 'LogEveryLoss',\n",
       " 'MaxupPerSample',\n",
       " 'MaxupStandard',\n",
       " 'MultiCycleLR',\n",
       " 'NoiseRobustnessTest',\n",
       " 'Profile',\n",
       " 'ProfileAutograd',\n",
       " 'PruneLowMagnitude',\n",
       " 'PruneLowSNRGlobal',\n",
       " 'PruneLowSNRLayers',\n",
       " 'RegularizeLoss',\n",
       " 'ReportMaxAccuracy',\n",
       " 'RezeroWeights',\n",
       " 'SaveFinalCheckpoint',\n",
       " 'StepBasedLogging',\n",
       " 'TorchProfilerMixin',\n",
       " 'TrackRepresentationSparsity',\n",
       " 'UpdateBoostStrength',\n",
       " 'VaryBatchSize',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'composite_loss',\n",
       " 'configure_optimizer_param_groups',\n",
       " 'constrain_parameters',\n",
       " 'create_lr_test_experiment',\n",
       " 'cutmix',\n",
       " 'delay_load_checkpoint',\n",
       " 'export_model',\n",
       " 'extra_validations_per_epoch',\n",
       " 'gradient_metrics',\n",
       " 'inject_torch_profiler_mixin',\n",
       " 'knowledge_distillation',\n",
       " 'legacy_imagenet_config',\n",
       " 'load_preprocessed_data',\n",
       " 'log_backprop_structure',\n",
       " 'log_covariance',\n",
       " 'log_every_learning_rate',\n",
       " 'log_every_loss',\n",
       " 'lr_range_test',\n",
       " 'maxup',\n",
       " 'multi_cycle_lr',\n",
       " 'noise_robustness_test',\n",
       " 'profile',\n",
       " 'profile_autograd',\n",
       " 'prune_low_magnitude',\n",
       " 'prune_low_snr',\n",
       " 'regularize_loss',\n",
       " 'report_max_accuracy',\n",
       " 'rezero_weights',\n",
       " 'save_final_checkpoint',\n",
       " 'step_based_logging',\n",
       " 'torch_profiler',\n",
       " 'track_representation_sparsity',\n",
       " 'update_boost_strength',\n",
       " 'vary_batch_size']"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "dir(mixins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "INFO:SupervisedExperiment:Execution order: {'complexity_loss': [],\n",
      " 'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      " 'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      " 'create_model': ['SupervisedExperiment.create_model'],\n",
      " 'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      " 'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      " 'create_train_sampler': ['SupervisedExperiment.create_train_sampler'],\n",
      " 'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      " 'create_validation_sampler': ['SupervisedExperiment.create_validation_sampler'],\n",
      " 'error_loss': ['SupervisedExperiment.error_loss'],\n",
      " 'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      " 'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'insert_pre_experiment_result': [],\n",
      " 'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      " 'post_batch': ['SupervisedExperiment: Logging'],\n",
      " 'post_epoch': [],\n",
      " 'post_optimizer_step': [],\n",
      " 'pre_batch': [],\n",
      " 'pre_epoch': [],\n",
      " 'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      " 'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      " 'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      " 'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      " 'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "                      'SupervisedExperiment.setup_experiment'],\n",
      " 'should_stop': ['SupervisedExperiment.should_stop'],\n",
      " 'stop_experiment': [],\n",
      " 'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      " 'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      " 'transform_model': ['SupervisedExperiment.transform_model'],\n",
      " 'validate': ['SupervisedExperiment.validate']}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch: 1 Acc: 0.00\n",
      "{'total_correct': 0, 'total_tested': 0, 'mean_loss': 0.0, 'mean_accuracy': 0.0, 'learning_rate': 0.01}\n",
      "Epoch: 2 Acc: 0.00\n",
      "{'total_correct': 0, 'total_tested': 0, 'mean_loss': 0.0, 'mean_accuracy': 0.0, 'learning_rate': 0.01}\n",
      "Epoch: 3 Acc: 0.15\n",
      "{'total_correct': 49, 'total_tested': 320, 'mean_loss': 3.5604583740234377, 'mean_accuracy': 0.153125, 'learning_rate': 0.01}\n",
      "Epoch: 4 Acc: 0.19\n",
      "{'total_correct': 60, 'total_tested': 320, 'mean_loss': 3.025089645385742, 'mean_accuracy': 0.1875, 'learning_rate': 0.01}\n",
      "Epoch: 5 Acc: 0.27\n",
      "{'total_correct': 86, 'total_tested': 320, 'mean_loss': 2.1711780548095705, 'mean_accuracy': 0.26875, 'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# models and datasets available - not part of Vernon\n",
    "from nupic.research.frameworks.pytorch import models as local_models\n",
    "from nupic.research.frameworks.pytorch import datasets as local_datasets\n",
    "\n",
    "supervised_test_v3 = deepcopy(supervised_test)\n",
    "supervised_test_v3.update(\n",
    "    # dataset - alternative using torchvision factory\n",
    "    # includes base transforms as transforming to tensor and normalization\n",
    "    dataset_class=local_datasets.torchvisiondataset,\n",
    "    dataset_args=dict(root=\"~/nta/datasets\", dataset_name=\"CIFAR10\"),\n",
    "    # can use local models available\n",
    "    model_class=local_models.resnet9,\n",
    "    model_args=dict(num_classes=10),\n",
    ")\n",
    "\n",
    "run(SupervisedExperiment, config=supervised_test_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['ContextDependentPermutedMNIST',\n",
       " 'PermutedMNIST',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'download_gsc_data',\n",
       " 'gsc_factory',\n",
       " 'imagenet',\n",
       " 'imagenet_factory',\n",
       " 'omniglot',\n",
       " 'permuted_mnist',\n",
       " 'preprocessed_gsc',\n",
       " 'torchvision_factory',\n",
       " 'torchvisiondataset']"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "dir(local_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['DenseNetCIFAR',\n",
       " 'LeSparseNet',\n",
       " 'MobileNetV1',\n",
       " 'ModifiedInitStandardMLP',\n",
       " 'NoSoDenseNetCIFAR',\n",
       " 'ResNet',\n",
       " 'SparseMLP',\n",
       " 'StandardMLP',\n",
       " 'VGGSparseNet',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'common_models',\n",
       " 'le_sparse_net',\n",
       " 'mobile_net_v1_sparse_depth',\n",
       " 'mobile_net_v1_sparse_point',\n",
       " 'mobilenetv1',\n",
       " 'not_so_densenet',\n",
       " 'resnet9',\n",
       " 'resnet_models',\n",
       " 'resnets',\n",
       " 'separable_convolution2d',\n",
       " 'vgg19_dense_net',\n",
       " 'vgg19_sparse_net',\n",
       " 'vgg_sparse_net']"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "dir(local_models)"
   ]
  },
  {
   "source": [
    "## Continual Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-12-13 15:52:01,091\tINFO resource_spec.py:223 -- Starting Ray with 9.91 GiB memory available for workers and up to 4.97 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "{'checkpoint_at_end': False,\n",
      " 'checkpoint_freq': 0,\n",
      " 'checkpoint_score_attr': None,\n",
      " 'config': {'batch_size': 32,\n",
      "            'batches_in_epoch': 30,\n",
      "            'batches_in_epoch_val': 30,\n",
      "            'dataset_args': {'dataset_name': 'MNIST', 'root': '~/nta/datasets'},\n",
      "            'dataset_class': <function torchvisiondataset at 0x7fd310339790>,\n",
      "            'dist_port': 58481,\n",
      "            'distributed': False,\n",
      "            'epochs': 5,\n",
      "            'epochs_to_validate': [],\n",
      "            'evaluation_metrics': ['eval_current_task',\n",
      "                                   'eval_all_visited_tasks'],\n",
      "            'experiment_class': <class '__main__.ReduceLRContinualLearningExperiment'>,\n",
      "            'log_level': 'INFO',\n",
      "            'model_args': {'hidden_sizes': (50, 50, 50),\n",
      "                           'input_size': (28, 28),\n",
      "                           'num_classes': 10},\n",
      "            'model_class': <class 'nupic.research.frameworks.pytorch.models.common_models.StandardMLP'>,\n",
      "            'num_classes': 10,\n",
      "            'num_gpus': 0,\n",
      "            'num_tasks': 5,\n",
      "            'optimizer_args': {'lr': 0.01, 'momentum': 0.9, 'nesterov': False},\n",
      "            'optimizer_class': <class 'torch.optim.sgd.SGD'>,\n",
      "            'reuse_actors': False,\n",
      "            'workers': 4},\n",
      " 'export_formats': None,\n",
      " 'fail_fast': False,\n",
      " 'global_checkpoint_period': 10,\n",
      " 'keep_checkpoints_num': None,\n",
      " 'local_dir': None,\n",
      " 'loggers': None,\n",
      " 'max_failures': 0,\n",
      " 'name': None,\n",
      " 'num_samples': 1,\n",
      " 'progress_reporter': None,\n",
      " 'queue_trials': False,\n",
      " 'raise_on_failed_trial': True,\n",
      " 'ray_auto_init': True,\n",
      " 'resources_per_trial': None,\n",
      " 'restore': None,\n",
      " 'resume': False,\n",
      " 'return_trials': False,\n",
      " 'reuse_actors': False,\n",
      " 'run_or_experiment': <class 'nupic.research.frameworks.ray.trainables.RemoteProcessTrainable'>,\n",
      " 'scheduler': None,\n",
      " 'search_alg': None,\n",
      " 'server_port': 4321,\n",
      " 'stop': {},\n",
      " 'sync_on_checkpoint': True,\n",
      " 'sync_to_cloud': None,\n",
      " 'sync_to_driver': None,\n",
      " 'trial_executor': None,\n",
      " 'trial_name_creator': None,\n",
      " 'upload_dir': None,\n",
      " 'verbose': 2,\n",
      " 'with_server': False}\n",
      "2021-12-13 15:52:02,424\tINFO services.py:1191 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 17.4/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/16 CPUs, 0/0 GPUs, 0.0/9.91 GiB heap, 0.0/3.42 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                        </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_abcf5_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Execution order: {'aggregate_pre_experiment_results': ['ContinualLearningExperiment.aggregate_pre_experiment_results'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'aggregate_results': ['ContinualLearningExperiment.aggregate_results'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'complexity_loss': [],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'create_loaders': ['SupervisedExperiment.create_loaders'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'create_model': ['SupervisedExperiment.create_model'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'create_train_dataloader': ['SupervisedExperiment.create_train_dataloader'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'create_train_sampler': ['ContinualLearningExperiment.create_train_sampler'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'create_validation_dataloader': ['SupervisedExperiment.create_validation_dataloader'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'create_validation_sampler': ['ContinualLearningExperiment.create_validation_sampler'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'error_loss': ['SupervisedExperiment.error_loss'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'insert_pre_experiment_result': [],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'post_batch': ['SupervisedExperiment: Logging'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'post_epoch': [],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'post_optimizer_step': [],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'pre_batch': [],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'pre_epoch': [],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'run_epoch': ['SupervisedExperiment.run_epoch'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'run_iteration': ['ContinualLearningExperiment: Call run_task'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'run_task': ['Reduce LR for all weights',\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m               'ContinualLearningExperiment.run_task'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m                       'SupervisedExperiment.setup_experiment',\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m                       'ContinualLearningExperiment.setup_experiment',\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m                       'Freeze after task params'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'should_stop': ['ContinualLearningExperiment.should_stop'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'stop_experiment': [],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'transform_model': ['SupervisedExperiment.transform_model'],\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m  'validate': ['SupervisedExperiment.validate']}\n",
      "\u001b[2m\u001b[36m(pid=70082)\u001b[0m 2021-12-13 15:52:17,519\tWARNING trainable.py:771 -- Trainable._setup is deprecated and will be removed in a future version of Ray. Override Trainable.setup instead.\n",
      "\u001b[2m\u001b[36m(pid=70082)\u001b[0m 2021-12-13 15:52:17,519\tINFO trainable.py:248 -- Trainable.setup took 11.227 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=70082)\u001b[0m INFO:RemoteProcessTrainable:Pre-Experiment Result: None\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Training...\n",
      "Result for RemoteProcessTrainable_abcf5_00000:\n",
      "  date: 2021-12-13_15-52-25\n",
      "  done: false\n",
      "  eval_all_visited_tasks__mean_accuracy: 0.9979166666666667\n",
      "  eval_all_visited_tasks__mean_loss: 0.005142629643281301\n",
      "  eval_all_visited_tasks__total_correct: 958\n",
      "  eval_all_visited_tasks__total_tested: 960\n",
      "  eval_current_task__mean_accuracy: 0.9989583333333333\n",
      "  eval_current_task__mean_loss: 0.0018897388130426408\n",
      "  eval_current_task__total_correct: 959\n",
      "  eval_current_task__total_tested: 960\n",
      "  experiment_id: 21b341eacbdb421d92401089d7b49e13\n",
      "  experiment_tag: '0'\n",
      "  hostname: Lucas-MacBook-Pro.local\n",
      "  iterations_since_restore: 1\n",
      "  learning_rate: 0.01\n",
      "  node_ip: 192.168.0.10\n",
      "  pid: 70082\n",
      "  time_since_restore: 7.503525972366333\n",
      "  time_this_iter_s: 7.503525972366333\n",
      "  time_total_s: 7.503525972366333\n",
      "  timestamp: 1639439545\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: abcf5_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 17.6/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/16 CPUs, 0/0 GPUs, 0.0/9.91 GiB heap, 0.0/3.42 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                        </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_abcf5_00000</td><td>RUNNING </td><td>192.168.0.10:70082</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.50353</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=70082)\u001b[0m INFO:RemoteProcessTrainable:End Iteration Result: {'learning_rate': 0.01}\n",
      "\u001b[2m\u001b[36m(pid=70082)\u001b[0m 2021-12-13 15:52:25,025\tWARNING trainable.py:639 -- Trainable._train is deprecated and will be removed in a future version of Ray. Override Trainable.step instead.\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Setting learning rate to 0.00\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Training...\n",
      "Result for RemoteProcessTrainable_abcf5_00000:\n",
      "  date: 2021-12-13_15-52-32\n",
      "  done: false\n",
      "  eval_all_visited_tasks__mean_accuracy: 0.48541666666666666\n",
      "  eval_all_visited_tasks__mean_loss: 2.90626703898112\n",
      "  eval_all_visited_tasks__total_correct: 466\n",
      "  eval_all_visited_tasks__total_tested: 960\n",
      "  eval_current_task__mean_accuracy: 0.9614583333333333\n",
      "  eval_current_task__mean_loss: 0.14333314895629884\n",
      "  eval_current_task__total_correct: 923\n",
      "  eval_current_task__total_tested: 960\n",
      "  experiment_id: 21b341eacbdb421d92401089d7b49e13\n",
      "  experiment_tag: '0'\n",
      "  hostname: Lucas-MacBook-Pro.local\n",
      "  iterations_since_restore: 2\n",
      "  learning_rate: 0.001\n",
      "  node_ip: 192.168.0.10\n",
      "  pid: 70082\n",
      "  time_since_restore: 14.777796983718872\n",
      "  time_this_iter_s: 7.274271011352539\n",
      "  time_total_s: 14.777796983718872\n",
      "  timestamp: 1639439552\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: abcf5_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 17.6/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/16 CPUs, 0/0 GPUs, 0.0/9.91 GiB heap, 0.0/3.42 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                        </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_abcf5_00000</td><td>RUNNING </td><td>192.168.0.10:70082</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         14.7778</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=70082)\u001b[0m INFO:RemoteProcessTrainable:End Iteration Result: {'learning_rate': 0.001}\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Setting learning rate to 0.00\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Training...\n",
      "Result for RemoteProcessTrainable_abcf5_00000:\n",
      "  date: 2021-12-13_15-52-39\n",
      "  done: false\n",
      "  eval_all_visited_tasks__mean_accuracy: 0.296875\n",
      "  eval_all_visited_tasks__mean_loss: 6.7467803955078125\n",
      "  eval_all_visited_tasks__total_correct: 285\n",
      "  eval_all_visited_tasks__total_tested: 960\n",
      "  eval_current_task__mean_accuracy: 0.971875\n",
      "  eval_current_task__mean_loss: 0.1525710423787435\n",
      "  eval_current_task__total_correct: 933\n",
      "  eval_current_task__total_tested: 960\n",
      "  experiment_id: 21b341eacbdb421d92401089d7b49e13\n",
      "  experiment_tag: '0'\n",
      "  hostname: Lucas-MacBook-Pro.local\n",
      "  iterations_since_restore: 3\n",
      "  learning_rate: 0.001\n",
      "  node_ip: 192.168.0.10\n",
      "  pid: 70082\n",
      "  time_since_restore: 21.782018184661865\n",
      "  time_this_iter_s: 7.004221200942993\n",
      "  time_total_s: 21.782018184661865\n",
      "  timestamp: 1639439559\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: abcf5_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 17.6/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/16 CPUs, 0/0 GPUs, 0.0/9.91 GiB heap, 0.0/3.42 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                        </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_abcf5_00000</td><td>RUNNING </td><td>192.168.0.10:70082</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">          21.782</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=70082)\u001b[0m INFO:RemoteProcessTrainable:End Iteration Result: {'learning_rate': 0.001}\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Setting learning rate to 0.00\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Training...\n",
      "Result for RemoteProcessTrainable_abcf5_00000:\n",
      "  date: 2021-12-13_15-52-46\n",
      "  done: false\n",
      "  eval_all_visited_tasks__mean_accuracy: 0.234375\n",
      "  eval_all_visited_tasks__mean_loss: 7.244156901041666\n",
      "  eval_all_visited_tasks__total_correct: 225\n",
      "  eval_all_visited_tasks__total_tested: 960\n",
      "  eval_current_task__mean_accuracy: 0.9864583333333333\n",
      "  eval_current_task__mean_loss: 0.05039400259653727\n",
      "  eval_current_task__total_correct: 947\n",
      "  eval_current_task__total_tested: 960\n",
      "  experiment_id: 21b341eacbdb421d92401089d7b49e13\n",
      "  experiment_tag: '0'\n",
      "  hostname: Lucas-MacBook-Pro.local\n",
      "  iterations_since_restore: 4\n",
      "  learning_rate: 0.001\n",
      "  node_ip: 192.168.0.10\n",
      "  pid: 70082\n",
      "  time_since_restore: 28.78888511657715\n",
      "  time_this_iter_s: 7.006866931915283\n",
      "  time_total_s: 28.78888511657715\n",
      "  timestamp: 1639439566\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: abcf5_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 17.6/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/16 CPUs, 0/0 GPUs, 0.0/9.91 GiB heap, 0.0/3.42 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                        </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_abcf5_00000</td><td>RUNNING </td><td>192.168.0.10:70082</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         28.7889</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=70082)\u001b[0m INFO:RemoteProcessTrainable:End Iteration Result: {'learning_rate': 0.001}\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Setting learning rate to 0.00\n",
      "\u001b[2m\u001b[36m(pid=70081)\u001b[0m INFO:ReduceLRContinualLearningExperiment:Training...\n",
      "Result for RemoteProcessTrainable_abcf5_00000:\n",
      "  date: 2021-12-13_15-52-53\n",
      "  done: true\n",
      "  eval_all_visited_tasks__mean_accuracy: 0.18333333333333332\n",
      "  eval_all_visited_tasks__mean_loss: 8.25047353108724\n",
      "  eval_all_visited_tasks__total_correct: 176\n",
      "  eval_all_visited_tasks__total_tested: 960\n",
      "  eval_current_task__mean_accuracy: 0.9552083333333333\n",
      "  eval_current_task__mean_loss: 0.16957790056864422\n",
      "  eval_current_task__total_correct: 917\n",
      "  eval_current_task__total_tested: 960\n",
      "  experiment_id: 21b341eacbdb421d92401089d7b49e13\n",
      "  experiment_tag: '0'\n",
      "  hostname: Lucas-MacBook-Pro.local\n",
      "  iterations_since_restore: 5\n",
      "  learning_rate: 0.001\n",
      "  node_ip: 192.168.0.10\n",
      "  pid: 70082\n",
      "  time_since_restore: 35.99036931991577\n",
      "  time_this_iter_s: 7.201484203338623\n",
      "  time_total_s: 35.99036931991577\n",
      "  timestamp: 1639439573\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: abcf5_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 17.6/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/9.91 GiB heap, 0.0/3.42 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                        </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_abcf5_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         35.9904</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 17.6/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/9.91 GiB heap, 0.0/3.42 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                        </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_abcf5_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         35.9904</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "**** Trial ended\n"
     ]
    }
   ],
   "source": [
    "from nupic.research.frameworks.continual_learning.experiments import ContinualLearningExperiment\n",
    "from nupic.research.frameworks.pytorch.models import StandardMLP\n",
    "from nupic.research.frameworks.continual_learning.mixins import ReduceLRAfterTask\n",
    "from nupic.research.frameworks.pytorch.datasets import torchvisiondataset\n",
    "\n",
    "class ReduceLRContinualLearningExperiment(ReduceLRAfterTask,\n",
    "                                          ContinualLearningExperiment):\n",
    "    pass\n",
    "\n",
    "cl_mnist = dict(\n",
    "    # specific to continual learning\n",
    "    distributed=False,\n",
    "    experiment_class=ReduceLRContinualLearningExperiment,\n",
    "    num_classes=10,\n",
    "    num_tasks=5,\n",
    "    evaluation_metrics=[\n",
    "        \"eval_current_task\",\n",
    "        \"eval_all_visited_tasks\",\n",
    "    ],\n",
    "    # dataset\n",
    "    dataset_class=torchvisiondataset,\n",
    "    dataset_args=dict(root=\"~/nta/datasets\", dataset_name=\"MNIST\"),    \n",
    "    # regular experiments\n",
    "    model_class=StandardMLP,\n",
    "    model_args=dict(\n",
    "        input_size=(28, 28), num_classes=10, hidden_sizes=(50, 50, 50)\n",
    "    ),\n",
    "    # hyperparameters\n",
    "    epochs_to_validate=[],\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    optimizer_class=torch.optim.SGD,\n",
    "    optimizer_args=dict(lr=0.01, momentum=0.9, nesterov=False),\n",
    "    # for debugging\n",
    "    batches_in_epoch=30,\n",
    "    batches_in_epoch_val=30\n",
    ")\n",
    "\n",
    "from nupic.research.frameworks.ray.run_with_raytune import run_single_instance\n",
    "run_single_instance(cl_mnist)\n"
   ]
  },
  {
   "source": [
    "## Meta-Continual Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-12-13 16:01:17,050\tINFO resource_spec.py:223 -- Starting Ray with 10.06 GiB memory available for workers and up to 5.04 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "{'checkpoint_at_end': False,\n",
      " 'checkpoint_freq': 0,\n",
      " 'checkpoint_score_attr': None,\n",
      " 'config': {'batch_size': 5,\n",
      "            'dataset_args': {'root': '~/nta/datasets'},\n",
      "            'dataset_class': <function omniglot at 0x7fc3e847ac10>,\n",
      "            'dist_port': 59205,\n",
      "            'distributed': False,\n",
      "            'epochs': 2,\n",
      "            'experiment_class': <class 'nupic.research.frameworks.meta_continual_learning.experiments.meta_cl_experiment.MetaContinualLearningExperiment'>,\n",
      "            'fast_params': ['adaptation.*'],\n",
      "            'log_level': 'INFO',\n",
      "            'model_args': {'num_classes': 50},\n",
      "            'model_class': <class 'nupic.research.frameworks.meta_continual_learning.models.OMLNetwork'>,\n",
      "            'num_batches_train': 1,\n",
      "            'num_classes': 50,\n",
      "            'num_gpus': 0,\n",
      "            'num_tasks_per_epoch': 10,\n",
      "            'optimizer_args': {'lr': 0.0001},\n",
      "            'optimizer_class': <class 'torch.optim.adam.Adam'>,\n",
      "            'reuse_actors': False,\n",
      "            'test_train_params': ['adaptation.*'],\n",
      "            'workers': 4},\n",
      " 'export_formats': None,\n",
      " 'fail_fast': False,\n",
      " 'global_checkpoint_period': 10,\n",
      " 'keep_checkpoints_num': None,\n",
      " 'local_dir': None,\n",
      " 'loggers': None,\n",
      " 'max_failures': 0,\n",
      " 'name': None,\n",
      " 'num_samples': 1,\n",
      " 'progress_reporter': None,\n",
      " 'queue_trials': False,\n",
      " 'raise_on_failed_trial': True,\n",
      " 'ray_auto_init': True,\n",
      " 'resources_per_trial': None,\n",
      " 'restore': None,\n",
      " 'resume': False,\n",
      " 'return_trials': False,\n",
      " 'reuse_actors': False,\n",
      " 'run_or_experiment': <class 'nupic.research.frameworks.ray.trainables.RemoteProcessTrainable'>,\n",
      " 'scheduler': None,\n",
      " 'search_alg': None,\n",
      " 'server_port': 4321,\n",
      " 'stop': {},\n",
      " 'sync_on_checkpoint': True,\n",
      " 'sync_to_cloud': None,\n",
      " 'sync_to_driver': None,\n",
      " 'trial_executor': None,\n",
      " 'trial_name_creator': None,\n",
      " 'upload_dir': None,\n",
      " 'verbose': 2,\n",
      " 'with_server': False}\n",
      "2021-12-13 16:01:17,505\tINFO services.py:1191 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 17.1/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/16 CPUs, 0/0 GPUs, 0.0/10.06 GiB heap, 0.0/3.47 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                        </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_f6a9a_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m INFO:MetaContinualLearningExperiment:Execution order: {'adapt': ['MetaContinualLearningExperiment.adapt'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'clone_model': ['MetaContinualLearningExperiment.clone_model'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'complexity_loss': [],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'create_loaders': ['MetaContinualLearningExperiment.create_loaders'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'create_lr_scheduler': ['SupervisedExperiment.create_lr_scheduler'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'create_model': ['SupervisedExperiment.create_model'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'create_optimizer': ['SupervisedExperiment.create_optimizer'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'create_replay_dataloader': ['MetaContinualLearningExperiment.create_replay_dataloader'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'create_replay_sampler': ['MetaContinualLearningExperiment.create_replay_sampler'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'create_sampler': ['MetaContinualLearningExperiment.create_sampler'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'create_slow_train_dataloader': ['MetaContinualLearningExperiment.create_slow_train_dataloader'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'create_task_sampler': ['MetaContinualLearningExperiment.create_task_sampler'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'create_train_dataloader': ['MetaContinualLearningExperiment.create_train_dataloader'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'create_train_sampler': ['MetaContinualLearningExperiment.create_train_sampler'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'create_validation_dataloader': ['MetaContinualLearningExperiment.create_validation_dataloader'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'create_validation_sampler': ['MetaContinualLearningExperiment.create_validation_sampler'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'error_loss': ['SupervisedExperiment.error_loss'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'get_model': ['MetaContinualLearningExperiment.get_model'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'get_named_fast_params': ['MetaContinualLearningExperiment.get_named_fast_params'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'get_readable_result': ['SupervisedExperiment: Basic keys'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'get_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'insert_pre_experiment_result': [],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'load_dataset': ['SupervisedExperiment.load_dataset'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'post_batch': ['SupervisedExperiment: Logging'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'post_epoch': [],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'post_optimizer_step': ['MetaContinualLearningExperiment.post_optimizer_step'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'pre_batch': [],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'pre_epoch': [],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'pre_task': ['MetaContinualLearningExperiment.pre_task'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'run_epoch': ['MetaContinualLearningExperiment.run_epoch'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'run_iteration': ['SupervisedExperiment.run_iteration'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'run_pre_experiment': ['SupervisedExperiment.run_pre_experiment'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'run_task': ['MetaContinualLearningExperiment.run_task'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'sample_slow_data': ['MetaContinualLearningExperiment.sample_slow_data'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'set_state': ['SupervisedExperiment: Model, optimizer, LR scheduler, epoch'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'setup_experiment': ['ExperimentBase: Initialize logger',\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m                       'SupervisedExperiment.setup_experiment'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'should_stop': ['SupervisedExperiment.should_stop'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'stop_experiment': [],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'train_epoch': ['SupervisedExperiment.train_epoch'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'transform_data_to_device': ['SupervisedExperiment.transform_data_to_device'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'transform_model': ['SupervisedExperiment.transform_model'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'update_params': ['MetaContinualLearningExperiment.update_params'],\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m  'validate': ['SupervisedExperiment.validate']}\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=70569)\u001b[0m 2021-12-13 16:01:34,457\tWARNING trainable.py:771 -- Trainable._setup is deprecated and will be removed in a future version of Ray. Override Trainable.setup instead.\n",
      "\u001b[2m\u001b[36m(pid=70569)\u001b[0m 2021-12-13 16:01:34,457\tINFO trainable.py:248 -- Trainable.setup took 13.448 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=70569)\u001b[0m INFO:RemoteProcessTrainable:Pre-Experiment Result: None\n",
      "\u001b[2m\u001b[36m(pid=70573)\u001b[0m INFO:MetaContinualLearningExperiment:Setup: fast_param_names=['adaptation.0.weight', 'adaptation.0.bias']\n",
      "Result for RemoteProcessTrainable_f6a9a_00000:\n",
      "  date: 2021-12-13_16-03-00\n",
      "  done: false\n",
      "  experiment_id: 8df281c6cf984ccfa456a6da7ceb4e14\n",
      "  experiment_tag: '0'\n",
      "  hostname: Lucas-MacBook-Pro.local\n",
      "  iterations_since_restore: 1\n",
      "  learning_rate: 0.0001\n",
      "  mean_accuracy: 0.10144927536231885\n",
      "  mean_loss: 71.38861846923828\n",
      "  neg_mean_loss: -71.38861846923828\n",
      "  node_ip: 192.168.0.10\n",
      "  pid: 70569\n",
      "  time_since_restore: 85.68898892402649\n",
      "  time_this_iter_s: 85.68898892402649\n",
      "  time_total_s: 85.68898892402649\n",
      "  timestamp: 1639440180\n",
      "  timesteps_since_restore: 0\n",
      "  total_correct: 7\n",
      "  total_tested: 69\n",
      "  training_iteration: 1\n",
      "  trial_id: f6a9a_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 17.8/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/16 CPUs, 0/0 GPUs, 0.0/10.06 GiB heap, 0.0/3.47 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                        </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_f6a9a_00000</td><td>RUNNING </td><td>192.168.0.10:70569</td><td style=\"text-align: right;\">0.101449</td><td style=\"text-align: right;\">71.3886</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          85.689</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=70569)\u001b[0m INFO:RemoteProcessTrainable:End Iteration Result: {'total_correct': 7, 'total_tested': 69, 'learning_rate': 0.0001, 'validation_loss': 71.38861846923828, 'validation_accuracy': 0.10144927536231885}\n",
      "\u001b[2m\u001b[36m(pid=70569)\u001b[0m 2021-12-13 16:03:00,149\tWARNING trainable.py:639 -- Trainable._train is deprecated and will be removed in a future version of Ray. Override Trainable.step instead.\n",
      "Result for RemoteProcessTrainable_f6a9a_00000:\n",
      "  date: 2021-12-13_16-04-00\n",
      "  done: true\n",
      "  experiment_id: 8df281c6cf984ccfa456a6da7ceb4e14\n",
      "  experiment_tag: '0'\n",
      "  hostname: Lucas-MacBook-Pro.local\n",
      "  iterations_since_restore: 2\n",
      "  learning_rate: 0.0001\n",
      "  mean_accuracy: 0.10144927536231885\n",
      "  mean_loss: 28.799962997436523\n",
      "  neg_mean_loss: -28.799962997436523\n",
      "  node_ip: 192.168.0.10\n",
      "  pid: 70569\n",
      "  time_since_restore: 145.80434799194336\n",
      "  time_this_iter_s: 60.11535906791687\n",
      "  time_total_s: 145.80434799194336\n",
      "  timestamp: 1639440240\n",
      "  timesteps_since_restore: 0\n",
      "  total_correct: 7\n",
      "  total_tested: 69\n",
      "  training_iteration: 2\n",
      "  trial_id: f6a9a_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 17.7/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.06 GiB heap, 0.0/3.47 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                        </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_f6a9a_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.101449</td><td style=\"text-align: right;\">  28.8</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         145.804</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=70569)\u001b[0m INFO:RemoteProcessTrainable:End Iteration Result: {'total_correct': 7, 'total_tested': 69, 'learning_rate': 0.0001, 'validation_loss': 28.799962997436523, 'validation_accuracy': 0.10144927536231885}\n\u001b[2m\u001b[36m(pid=70569)\u001b[0m 2021-12-13 16:04:00,278\tWARNING trainable.py:821 -- Trainable._stop is deprecated and will be removed in a future version of Ray. Override Trainable.cleanup instead.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 17.7/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.06 GiB heap, 0.0/3.47 GiB objects<br>Result logdir: /Users/lsouza/ray_results/RemoteProcessTrainable<br>Number of trials: 1 (1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                        </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n</thead>\n<tbody>\n<tr><td>RemoteProcessTrainable_f6a9a_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.101449</td><td style=\"text-align: right;\">  28.8</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         145.804</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "**** Trial ended\n"
     ]
    }
   ],
   "source": [
    "# ray running in local, if bug, needs to manually shutdown # FIXME\n",
    "import ray\n",
    "ray.shutdown()\n",
    "\n",
    "# networks and datasets under pytorch\n",
    "import torch\n",
    "from nupic.research.frameworks.meta_continual_learning.experiments import MetaContinualLearningExperiment\n",
    "from nupic.research.frameworks.pytorch.datasets import omniglot\n",
    "from nupic.research.frameworks.meta_continual_learning.models import OMLNetwork\n",
    "from nupic.research.frameworks.ray.run_with_raytune import run_single_instance\n",
    "\n",
    "meta_cl_omniglot = dict(\n",
    "    # experiment\n",
    "    experiment_class=MetaContinualLearningExperiment,\n",
    "    distributed=False,\n",
    "    # dataset\n",
    "    dataset_class=omniglot,\n",
    "    dataset_args=dict(root=\"~/nta/datasets\"),\n",
    "    # model\n",
    "    model_class=OMLNetwork,\n",
    "    model_args=dict(num_classes=50),\n",
    "    fast_params=[\"adaptation.*\"],\n",
    "    test_train_params=[\"adaptation.*\"],\n",
    "    # hyperparameters\n",
    "    batch_size=5,\n",
    "    num_batches_train=1,\n",
    "    epochs=2,\n",
    "    num_tasks_per_epoch=10,\n",
    "    num_classes=50,\n",
    "    optimizer_args=dict(lr=1e-4),\n",
    "    optimizer_class=torch.optim.Adam,\n",
    ")\n",
    "\n",
    "# Use one of the existing run functions\n",
    "run_single_instance(meta_cl_omniglot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}